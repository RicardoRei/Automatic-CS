{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Clustering\n",
    "\n",
    "To test and compare our dual encoders in a real setup we cannot use all the possible answers in our train set as candidate answers during prediction. For that reason in this notebook we will cluster the answers and select a smaller number of exemplars from those clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which dataset you want to cluster\n",
    "dataset = \"pinterest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import numpy as np\n",
    "import pickle\n",
    "word2ix =  pickle.load(open('../data/'+dataset+'/tmp/word2ix.pkl', 'rb'))\n",
    "trn_q_vecs, trn_a_vecs, trn_y = pickle.load(open('../data/'+dataset+'/tmp/de_train.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider only unique answers...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_answers = trn_a_vecs[trn_y == 1]\n",
    "unique_ans = [tuple(row) for row in train_answers]\n",
    "unique_ans = np.unique(unique_ans)\n",
    "possible_ans = [np.array(ans) for ans in unique_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    7,   57,  306,    7, 1618,    6,   10,  121,  363,    4,\n",
       "         96,   69,   66,   16,    8,   67,   87,   50,  306])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_ans[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform vectorized documents back into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devectorize(vec_docs, word2ix):\n",
    "    ix2word = {v:k for k,v in word2ix.items()}\n",
    "    docs = []\n",
    "    for vec in vec_docs:\n",
    "        docs.append([ix2word[ix] for ix in vec])\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = devectorize(possible_ans, word2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_UNK_', 'i', 'am', 'maggie', 'i', 'responded', 'to', 'your', 'other', 'ticket', '.', 'let', 'us', 'know', 'if', 'you', 'need', 'more', 'help', 'maggie']\n"
     ]
    }
   ],
   "source": [
    "print (documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means++\n",
    "Now that we have our documents back we will use K-Means++ to group similar answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class AnswerClustering(object):\n",
    "    \"\"\" Class that runs the K-Means++ in a set of answers. \"\"\"\n",
    "    def __init__(self, answers, matrix, vocabulary, max_iter=300, n_init=10):\n",
    "        self.answers = answers\n",
    "        self.matrix = matrix\n",
    "        self.vocabulary = vocabulary\n",
    "        self.max_iter = max_iter\n",
    "        self.n_init= n_init\n",
    "\n",
    "        # centroid-features matrix containing the centroids values per line\n",
    "        self.centroids = None\n",
    "        # document-centroid matrix\n",
    "        self.distances_to_centroids = None\n",
    "        # array where entry i contains the index of the answer closest to the centroid i\n",
    "        self.closest_document = None\n",
    "        # array where entry i contains the centroid that was assigned to i\n",
    "        self.predictions = None\n",
    "        \n",
    "    def clustering(self, n_clusters, minibatch=False):\n",
    "        \"\"\" Runs K-Means++ from the document-feature matrix loaded at initialization time. \"\"\"\n",
    "        self.k = n_clusters\n",
    "        if minibatch:\n",
    "            self.model = MiniBatchKMeans(n_clusters=n_clusters, max_iter=self.max_iter, n_init=self.n_init, init_size=500)\n",
    "        else:    \n",
    "            self.model = KMeans(n_clusters=n_clusters, max_iter=self.max_iter, n_init=self.n_init)\n",
    "        # document-centroid matrix\n",
    "        self.predictions = self.model.fit_predict(self.matrix)\n",
    "        centroids = self.model.cluster_centers_\n",
    "        self.closest_document = np.zeros(self.k)\n",
    "        for i in tqdm(range(self.k)):\n",
    "            min_dist = math.inf\n",
    "            ans_idx = None\n",
    "            centroid_vec = centroids[i]\n",
    "            for j in range (len(self.answers)):\n",
    "                ans_vec = self.matrix[j, :]\n",
    "                distance = euclidean_distances([centroid_vec], ans_vec)[0][0]\n",
    "                if self.predictions[j] == i and distance < min_dist:\n",
    "                    min_dist = distance\n",
    "                    ans_idx = j\n",
    "            self.closest_document[i] = ans_idx\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \"\"\" The silhouette_score gives the average value for all the samples.\n",
    "            This gives a perspective into the density and separation of the formed clusters.\n",
    "        \"\"\"\n",
    "        self.silhouette_avg = silhouette_score(self.matrix, self.predictions)\n",
    "        self.sample_silhouette_values = silhouette_samples(self.matrix, self.predictions)\n",
    "        self.clusters_avg_silhouette = np.zeros(self.k)\n",
    "        for i in range(self.k):\n",
    "            ith_cluster_silhouette_values = self.sample_silhouette_values[self.predictions == i]\n",
    "            self.clusters_avg_silhouette[i] = ith_cluster_silhouette_values.mean()\n",
    "        return (self.silhouette_avg, self.clusters_avg_silhouette, self.sample_silhouette_values)\n",
    "\n",
    "    def find_best_k(self, k_min, k_max, minibatch=False):\n",
    "        \"\"\" Runs K-Means for several K values in order to find the one that maximizes the silhouette average score.\n",
    "            Also creates a plot with the silhouette score per K and ends by running a final K-Means for the best K values.\n",
    "        \"\"\"\n",
    "        silhouette_values = np.zeros(k_max - k_min)\n",
    "        for k in tqdm(range(k_min, k_max)):\n",
    "            self.clustering(k, minibatch)\n",
    "            silhouette_avg = silhouette_score(self.matrix, self.predictions)\n",
    "            silhouette_values[k - k_min] = silhouette_avg\n",
    "        best_k = silhouette_values.argmax() + k_min\n",
    "\n",
    "        self.clustering(best_k, minibatch)\n",
    "        plt.plot([k for k in range(k_min, k_max)], silhouette_values)\n",
    "        plt.ylim((-1, 1))\n",
    "        plt.ylabel('Silhouette Clustering Score')\n",
    "        plt.xlabel('Values of K')\n",
    "        plt.show()\n",
    "        \n",
    "    def dump_data_to_dict(self):\n",
    "        \"\"\" Bluids a dict that associates to each cluster a list of attributes and then saves it.\n",
    "            (e.g: clusters[i] is a list containing the elements inside cluster i and some information about the cluster)\n",
    "        \"\"\"\n",
    "        clusters = []\n",
    "        order_centroids = self.model.cluster_centers_.argsort()[:, ::-1]\n",
    "        for label in range(self.k):\n",
    "            cluster_answers = []            \n",
    "            for i in range(len(self.predictions)):\n",
    "                if self.predictions[i] == label:\n",
    "                    cluster_answers.append(self.answers[i])\n",
    "            keywords = []\n",
    "            for idx in order_centroids[label, :5]:\n",
    "                keywords.append(self.vocabulary[idx])\n",
    "\n",
    "            # we consider only clusters with silhouette score higher then 0 and with more than 30 representatives.\n",
    "            cluster_info = {\"size\": len(cluster_answers), \"id\":label}\n",
    "            cluster_info[\"silhouette_avg\"] = self.clusters_avg_silhouette[label].item()\n",
    "            cluster_info[\"keywords\"] = keywords\n",
    "            cluster_info[\"answers\"] = cluster_answers\n",
    "            cluster_info[\"representative\"] = [self.answers[self.closest_document[label].astype(np.int64)]]\n",
    "            clusters.append(cluster_info)\n",
    "        self.cluster_data = clusters\n",
    "        return self.cluster_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Our documents are prepared to be used by a neural network but since we plan to cluster the answers with a simple TF-IDF feature extractor we first need to clean some things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_docs = [' '.join(sample) for sample in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_UNK_ i am maggie i responded to your other ticket . let us know if you need more help maggie'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ricardorei/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/ricardorei/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "def normalize_texts(docs):\n",
    "    norm_docs = []\n",
    "    for s in docs:\n",
    "        s = re.sub(r'_UNK_', '', s)\n",
    "        s = re.sub(r'_URL_', '', s)\n",
    "        s = re.sub(r'_EOT_', '', s)\n",
    "        s = re.sub(r'_USERID_', '', s)\n",
    "        norm_docs.append(s)\n",
    "    return norm_docs\n",
    "\n",
    "def POSfiltering(docs):\n",
    "    \"\"\" Function that will apply a POS tagger and filter only verbs, nouns, adjectives and adverbs.\n",
    "    \"\"\"\n",
    "    pos_docs = []\n",
    "    print (\"Applying POS filtering:\")\n",
    "    for doc in tqdm((docs)):\n",
    "        filtered_doc = []\n",
    "        tokens = nltk.tokenize.word_tokenize(doc)\n",
    "        tokens = nltk.pos_tag(tokens, tagset='universal')\n",
    "        for token in tokens:\n",
    "            if token[1] == \"NOUN\" or token[1] == \"VERB\" or token[1] == \"ADV\" or token[1] == \"ADJ\":\n",
    "                filtered_doc.append(token[0])\n",
    "        pos_docs.append(\" \".join(filtered_doc))\n",
    "    return pos_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying POS filtering:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7d004c05be462c8fd6b1238ee6788f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15612), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "norm_text_docs = normalize_texts(text_docs)\n",
    "pos_text_docs = POSfiltering(norm_text_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_UNK_ i am maggie i responded to your other ticket . let us know if you need more help maggie\n",
      " i am maggie i responded to your other ticket . let us know if you need more help maggie\n",
      "i am maggie i responded other ticket let know need more help maggie\n"
     ]
    }
   ],
   "source": [
    "print (text_docs[0])\n",
    "print (norm_text_docs[0])\n",
    "print (pos_text_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction:\n",
    "Before running the K-Means++ we ned to represent our documents in a feature space. We will simply use a TF-IDF feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, strip_accents='ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = vectorizer.fit_transform(pos_text_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['absolutely',\n",
       " 'abstract',\n",
       " 'abuse',\n",
       " 'abut',\n",
       " 'ac',\n",
       " 'acc',\n",
       " 'acccount',\n",
       " 'acceder',\n",
       " 'accedere',\n",
       " 'accelerated']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have some words that could be stemmed but I believe this is enough for creating well defined clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AnswerClustering([' '.join(sample) for sample in documents], doc2vec, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a5d188c4e34583802baefcec3005ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clustering.clustering(1000, minibatch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score for the 1000-Means++: 0.13219783416168693\n"
     ]
    }
   ],
   "source": [
    "print (\"Silhouette score for the {}-Means++: {}\".format(clustering.k, clustering.evaluate()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = clustering.dump_data_to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../data/{}/clustering.json\".format(dataset), 'w') as output_file:\n",
    "    json.dump({\"clustering_quality\": clustering.silhouette_avg.item(), \"data\": clustering.cluster_data}, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Answer pool:\n",
    "Now that we have our data clustered we will create an answer pool with the representative of each cluster. This means that we need to prepare 1000 different answers to be used during inference by our dual encoders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json file\n",
    "import json\n",
    "json_data = json.loads(open('../data/{}/clustering.json'.format(dataset), 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_pool = [cluster[\"representative\"][0] for cluster in json_data[\"data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ans_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi _NAME_ it s perfectly understandable that you re very upset about what s happened . please give us some more time to troubleshoot this issue and i will keep you posted on this as soon as possible . appreciate your patience ! thanks samm'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_pool[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer pool vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(docs, vocab):\n",
    "    vec_docs = []\n",
    "    for doc in docs:\n",
    "        vec_doc = []\n",
    "        for o in doc:\n",
    "            try:\n",
    "                if o != ' ':\n",
    "                    vec_doc.append(vocab[o])\n",
    "            except KeyError:\n",
    "                vec_doc.append(vocab[\"_UNK_\"])\n",
    "        vec_docs.append(vec_doc)\n",
    "    return np.array(vec_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_pool_toks = [answer.split(' ') for answer in ans_pool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', '_NAME_', 'it', 's', 'perfectly', 'understandable', 'that', 'you', 're', 'very', 'upset', 'about', 'what', 's', 'happened', '.', 'please', 'give', 'us', 'some', 'more', 'time', 'to', 'troubleshoot', 'this', 'issue', 'and', 'i', 'will', 'keep', 'you', 'posted', 'on', 'this', 'as', 'soon', 'as', 'possible', '.', 'appreciate', 'your', 'patience', '!', 'thanks', 'samm']\n"
     ]
    }
   ],
   "source": [
    "print (ans_pool_toks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_pool_vecs = vectorize(ans_pool_toks, word2ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 38, 20, 59, 1781, 5603, 29, 8, 44, 153, 1440, 95, 61, 59, 474, 4, 41, 480, 69, 84, 87, 159, 6, 1657, 13, 120, 9, 7, 92, 282, 8, 758, 21, 13, 49, 278, 49, 186, 4, 465, 10, 263, 39, 113, 1357]\n"
     ]
    }
   ],
   "source": [
    "print (ans_pool_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ans_pool_vecs, open('../data/{}/tmp/ans_pool.pkl'.format(dataset), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ar-env",
   "language": "python",
   "name": "ar-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
