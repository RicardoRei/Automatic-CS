{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Preprocessing\n",
    "\n",
    "This notebook processes Twitter customer support corpus from kaggle.\n",
    "\n",
    "First of all you need to [download the original corpus](https://www.kaggle.com/thoughtvector/customer-support-on-twitter/activity) and save the \"twcs.csv\" file into the data/twitter/ folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load Data\n",
    "kaggle_df = pd.read_csv(\"../data/twitter/twcs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 500\n",
    "kaggle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kaggle_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity in the next steps lets convert the csv file to a dictionary that converts ids to tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "def csv2dict(dataframe):\n",
    "    id2tweet = {}\n",
    "    for index, row in tqdm(dataframe.iterrows(), total=len(dataframe)):\n",
    "        tweet_id, author_id, inbound, created_at, text, response_tweet_id, in_response_tweet_id = row\n",
    "        id2tweet[tweet_id] = {\"author_id\": author_id, \n",
    "                              \"inbound\": inbound,\n",
    "                              \"created_at\": created_at,\n",
    "                              \"text\": text,\n",
    "                              \"response_tweet_id\": response_tweet_id,\n",
    "                              \"in_response_tweet_id\": in_response_tweet_id}\n",
    "    return id2tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tweet = csv2dict(kaggle_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tweet[119237]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the preprocessing done by  [Hardalov et al. (2018)](https://arxiv.org/abs/1809.00303) we will select only the tweets related to the Apple support. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apple_tweets(id2tweet):\n",
    "    apple_tweets = []\n",
    "    for _id, body in tqdm(id2tweet.items()):\n",
    "        if body[\"author_id\"] == \"AppleSupport\":\n",
    "            apple_tweets.append({**{\"tweet_id\": _id}, **body})\n",
    "    return apple_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_tweets = get_apple_tweets(id2tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each apple tweet in the is a possible answers. We now only need to get the context that triggered that answer.\n",
    "\n",
    "Note: We will truncate the context to a max of 150 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "\n",
    "def get_tweet_context(tweets, id2tweet):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    qa_pairs = []\n",
    "    for tweet in tqdm(tweets):\n",
    "        context = []\n",
    "        current_tweet = tweet\n",
    "        while len(context) < 150 and not np.isnan(current_tweet[\"in_response_tweet_id\"]):\n",
    "            try: \n",
    "                previous_tweet = id2tweet[current_tweet[\"in_response_tweet_id\"]]\n",
    "                context += tokenizer.tokenize(previous_tweet[\"text\"]) + [\"eottoken\"]\n",
    "                current_tweet = previous_tweet\n",
    "            except KeyError:\n",
    "                break\n",
    "        # in this corpus we have some answers that have no context. We will not consider those\n",
    "        if len(context) > 0: \n",
    "            qa_pairs.append({\"context\": ' '.join(context), \"answer\": tweet[\"text\"], \"label\": 1, \"created_at\": tweet[\"created_at\"]})\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_pairs = get_tweet_context(apple_tweets, id2tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweet_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this apple tweets several of them redirect the user to the DM's and do not provide a clear answer. For this reason we will try to exclude those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_redirected_tweets(qa_pairs):\n",
    "    new_pairs = []\n",
    "    for pair in qa_pairs:\n",
    "        if \"DM\" not in pair[\"answer\"]:\n",
    "            new_pairs.append(pair)\n",
    "    return new_pairs\n",
    "\n",
    "def filter_nonenglish_tweets(qa_pairs):\n",
    "    new_pairs = []\n",
    "    for pair in qa_pairs:\n",
    "        if  \"We offer support via Twitter in English\" not in pair[\"answer\"]\\\n",
    "        and \"We offer support via Twitter in English\" not in pair[\"context\"] \\\n",
    "        and \"Twitter support is available in English\" not in pair[\"answer\"] \\\n",
    "        and \"Twitter support is available in English\" not in pair[\"context\"]:\n",
    "            new_pairs.append(pair)\n",
    "    return new_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pairs = filter_nonenglish_tweets(filter_redirected_tweets(tweet_pairs))\n",
    "len(filtered_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pairs[3:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in to what [Hardalov et al. (2018)](https://arxiv.org/abs/1809.00303) reported, we obtain 49k Context/Answer pairs!\n",
    "\n",
    "With this pairs we can start preparing our data to feed our models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_URL(s):\n",
    "    return re.sub('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/[a-zA-Z0-9]*', 'toberepalcedwithurltoken', s)\n",
    "\n",
    "def remove_user_id(s):\n",
    "    return re.sub('@[0-9]+', 'useridtoken', s)\n",
    "\n",
    "def remove_apple_id(s):\n",
    "    return re.sub('@AppleSupport', 'appleidtoken', s)\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = remove_URL(s)\n",
    "    s = remove_user_id(s)\n",
    "    s = remove_apple_id(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'toberepalcedwithurltoken', '_URL_', s)\n",
    "    s = re.sub(r'useridtoken', '_USERID_', s)\n",
    "    s = re.sub(r'appleidtoken', '_APPLE_', s)\n",
    "    s = re.sub(r'eottoken', '_EOT_', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in tqdm(filtered_pairs):\n",
    "    pair[\"context\"] = normalize_string(pair[\"context\"])\n",
    "    pair[\"answer\"] = normalize_string(pair[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pairs[3:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_analysis(pairs):\n",
    "    question_word_count, answer_word_count = 0, 0\n",
    "    question_max, answer_max = 0, 0\n",
    "    question_min, answer_min = 1000, 1000\n",
    "    tokenizer = TweetTokenizer()\n",
    "    for sample in pairs:\n",
    "        # we only need to tokenize the answer because the context was already tokenized and a simple\n",
    "        # srting split will do the work\n",
    "        answer = tokenizer.tokenize(sample[\"answer\"])\n",
    "        if len(sample[\"context\"].split()) > question_max:\n",
    "            question_max = len(sample[\"context\"].split())\n",
    "        if len(answer) > answer_max:\n",
    "            answer_max = len(answer)\n",
    "        if len(sample[\"context\"].split()) < question_min:\n",
    "            question_min = len(sample[\"context\"].split())\n",
    "        if len(answer) < answer_min:\n",
    "            answer_min = len(answer)\n",
    "        question_word_count += len(sample[\"context\"].split())\n",
    "        answer_word_count += len(answer)        \n",
    "    return question_word_count/len(pairs), answer_word_count/len(pairs), question_max, answer_max, question_min, answer_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_q_words, avg_a_words, q_max, a_max, q_min, a_min = corpus_analysis(filtered_pairs)\n",
    "avg_q_words, avg_a_words, q_max, a_max, q_min, a_min, len(filtered_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see there the min length value for the question is 0... we will filter those questions for lacking context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split\n",
    "\n",
    "Lets split the QA pairs into a train, validation and test. For the validation and test sets we will use tweets from the last 5 days of this dataset (Note: The dataset contains only tweets until 03 Dec 2017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "test_pairs = []\n",
    "for pair in filtered_pairs:\n",
    "    if pair[\"created_at\"].split()[1] == \"Dec\":\n",
    "        test_pairs.append(pair)\n",
    "    elif pair[\"created_at\"].split()[1] == \"Nov\" and pair[\"created_at\"].split()[2] > \"29\":\n",
    "        test_pairs.append(pair)\n",
    "    else:\n",
    "        train.append(pair)\n",
    "\n",
    "dev_size = int(0.5 * len(test_pairs))\n",
    "dev = test_pairs[:dev_size]\n",
    "test = test_pairs[-dev_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(test_pairs), len(dev), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative sampling\n",
    "Now that we have our pairs splitted into different sets we can build negative examples to train the Dual Encoders.\n",
    "\n",
    "Similar to the Pinterest data in order to avoid correct answers with negative labels we will compare the answers in a TF-IDF space when building the pairs.\n",
    "\n",
    "\n",
    "Example of common answers:\n",
    "- _USERID_ Here’s what you can do to work around the issue until it’s fixed in a future software update: _URL_\n",
    "- _USERID_  An update has been released to assist with this issue. If you haven’t yet, please back up your device and update it to iOS 11.1.1. For more info, check out: _URL_\n",
    "- _USERID_ iOS 11.1.1 was recently released and it includes a fix for autocorrect issues. Be sure to back up your device prior to updating, and let us know if the issue persists afterwards. How to back up: _URL_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, strip_accents='ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit([sample[\"answer\"] for sample in train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "def generate_encoders_data(pairs, vectorizer, racio=1):\n",
    "    encoders_data = []\n",
    "    for i in tqdm(range(len(pairs))):\n",
    "        sample = {\"label\": 1, \"context\": pairs[i][\"context\"], \"answer\": pairs[i][\"answer\"]}\n",
    "        encoders_data.append(sample.copy())\n",
    "        count = 0\n",
    "        while count < racio:\n",
    "            random_idx = np.random.randint(0, len(pairs))\n",
    "            if cosine_similarity(vectorizer.transform([pairs[i][\"answer\"]]), vectorizer.transform([pairs[random_idx][\"answer\"]])) < 0.85:\n",
    "                sample = {\"label\": 0, \"context\": pairs[i][\"context\"], \"answer\": pairs[random_idx][\"answer\"]}\n",
    "                encoders_data.append(sample.copy())\n",
    "                count += 1\n",
    "    return encoders_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_de_data = generate_encoders_data(train, vectorizer)\n",
    "dev_de_data = generate_encoders_data(dev, vectorizer)\n",
    "test_de_data = generate_encoders_data(test, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this coder if you want to save inspect the data into json.\n",
    "\"\"\"\n",
    "import json\n",
    "with open('../data/twitter/de-train.json', 'w') as outfile:\n",
    "    json.dump(train_de_data, outfile)\n",
    "\n",
    "with open('../data/twitter/de-dev.json', 'w') as outfile:\n",
    "    json.dump(dev_de_data, outfile)\n",
    "\n",
    "with open('../data/twitter/de-test.json', 'w') as outfile:\n",
    "    json.dump(test_de_data, outfile)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Data\n",
    "With the Dev and Test sets we will also create the data for the ranking task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = dev + test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(positive_samples) # number of positive examples that will be used to create the ranking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_data = []\n",
    "for sample in positive_samples:\n",
    "    ranking_batch = {\"context\": sample[\"context\"], \"candidates\": [sample[\"answer\"]]}\n",
    "    random_idxs = np.random.randint(0, len(positive_samples), 9)\n",
    "    for i in range(random_idxs.shape[0]):\n",
    "        ranking_batch[\"candidates\"].append(positive_samples[random_idxs[i]][\"answer\"])\n",
    "    ranking_data.append(ranking_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_data[2][\"context\"], ranking_data[2][\"candidates\"][0], ranking_data[2][\"candidates\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/twitter/ranking.json', 'w') as outfile:\n",
    "    json.dump(ranking_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint\n",
    "Load all the data that was computed in the cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "train_de_data = json.loads(open('../data/twitter/de-train.json', 'r').read())\n",
    "dev_de_data = json.loads(open('../data/twitter/de-dev.json', 'r').read())\n",
    "test_de_data = json.loads(open('../data/twitter/de-test.json', 'r').read())\n",
    "ranking_data = json.loads(open('../data/twitter/ranking.json', 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "now that we have our QA pairs formed, filtered and splitted into different sets we can start builduing our vocabulary and tokenize the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('../data/twitter/tmp/'):\n",
    "    os.makedirs('../data/twitter/tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(pairs):\n",
    "    # Remenber that we have already done tokenization for the context... \n",
    "    # this means that for the context we just need to split the string\n",
    "    c_toks = [sample[\"context\"].split() for sample in pairs]\n",
    "    tokenizer = TweetTokenizer()\n",
    "    a_toks = [tokenizer.tokenize(sample[\"answer\"]) for sample in pairs]\n",
    "    labels = [sample[\"label\"] for sample in pairs]\n",
    "    return c_toks, a_toks, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_c_toks, trn_a_toks, trn_y = tokenize_texts(train_de_data)\n",
    "dev_c_toks, dev_a_toks, dev_y = tokenize_texts(dev_de_data)\n",
    "test_c_toks, test_a_toks, test_y = tokenize_texts(test_de_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (trn_c_toks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary & Vectorization\n",
    "Now that we have everything tokenized we can build our vocabulary and vectorize everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def build_vocabulary(tokenized_trn_docs, max_vocab = 60000, min_freq = 1):\n",
    "    freq = Counter(p for o in (tokenized_trn_docs) for p in o)\n",
    "    words_list = [o for o, c in freq.most_common(max_vocab) if c > min_freq and o != ' '] # vocab ordered by frequency\n",
    "    words_list.insert(0, \"_EOS_\")\n",
    "    words_list.insert(0, \"_BOS_\")\n",
    "    words_list.insert(0, \"_UNK_\")\n",
    "    words_list.insert(0, \"_PAD_\")\n",
    "    vocabulary = {}\n",
    "    for word in words_list:\n",
    "        vocabulary[word] = len(vocabulary)\n",
    "    return freq, words_list, vocabulary\n",
    "frequencies, words_list, vocabulary = build_vocabulary(trn_c_toks+trn_a_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"vocabulary size: {}\".format(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Top 10 most common words:\", frequencies.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From {} questions and {} answers ({} documents) the vocabulary size is {}\".format(len(trn_c_toks), len(trn_a_toks), len(trn_c_toks+trn_a_toks), len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (words_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(vocabulary, open('../data/twitter/tmp/word2ix.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "With our vocabulary and our documents splitted into tokens we can represent our documents as arrays in which each entry represents the index of a specific word in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(docs, vocab):\n",
    "    vec_docs = []\n",
    "    for doc in docs:\n",
    "        vec_doc = []\n",
    "        for o in doc:\n",
    "            try:\n",
    "                if o != ' ':\n",
    "                    vec_doc.append(vocab[o])\n",
    "            except KeyError:\n",
    "                vec_doc.append(vocab[\"_UNK_\"])\n",
    "        vec_docs.append(vec_doc)\n",
    "    return np.array(vec_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_c_vecs = vectorize(trn_c_toks, vocabulary) \n",
    "trn_a_vecs = vectorize(trn_a_toks, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_c_vecs = vectorize(dev_c_toks, vocabulary)\n",
    "dev_a_vecs = vectorize(dev_a_toks, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_c_vecs = vectorize(test_c_toks, vocabulary)\n",
    "test_a_vecs = vectorize(test_a_toks, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (trn_c_vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Dual Encoders Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((np.array(trn_c_vecs), np.array(trn_a_vecs), trn_y), open('../data/twitter/tmp/de_train.pkl', 'wb'))\n",
    "pickle.dump((np.array(dev_c_vecs), np.array(dev_a_vecs), dev_y), open('../data/twitter/tmp/de_dev.pkl', 'wb'))\n",
    "pickle.dump((np.array(test_c_vecs), np.array(test_a_vecs), test_y), open('../data/twitter/tmp/de_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the ranking data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ranking_data(data):\n",
    "    q_toks = [sample[\"context\"].split() for sample in tqdm(data)]\n",
    "    tokenizer = TweetTokenizer()\n",
    "    c_toks = [[tokenizer.tokenize(candidate) for candidate in sample[\"candidates\"]] for sample in tqdm(data)]\n",
    "    return q_toks, c_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_toks, c_toks = tokenize_ranking_data(ranking_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vecs = vectorize(q_toks, vocabulary)\n",
    "candidate_vecs = [vectorize(candidates, vocabulary) for candidates in c_toks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1_inputs = []\n",
    "e2_inputs = []\n",
    "for i in range(context_vecs.shape[0]):\n",
    "    e1_inputs.append(np.stack((context_vecs[i] for k in range(10))))\n",
    "    e2_inputs.append(candidate_vecs[i])\n",
    "print (len(e1_inputs), len(e2_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((e1_inputs, e2_inputs), open('../data/twitter/tmp/ranking.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence Model Data\n",
    "For our sequece-to-sequence model we just need to select the positive samples from trein, dev and test sets and append the BOS and EOS tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_q_vecs, trn_a_vecs, trn_y = pickle.load(open('../data/twitter/tmp/de_train.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.nonzero(trn_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_src_seqs = trn_q_vecs[idxs[0]]\n",
    "trn_tgt_seqs = trn_a_vecs[idxs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_src_seqs.shape, trn_tgt_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (trn_src_seqs[0])\n",
    "print (trn_tgt_seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_seq2seq_tokens(sequences, bos_token, eos_token):\n",
    "    for seq in sequences:\n",
    "        seq.insert(0, bos_token)\n",
    "        seq.append(eos_token)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_src_seqs = add_seq2seq_tokens(trn_src_seqs, vocabulary[\"_BOS_\"], vocabulary[\"_EOS_\"])\n",
    "trn_tgt_seqs = add_seq2seq_tokens(trn_tgt_seqs, vocabulary[\"_BOS_\"], vocabulary[\"_EOS_\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (trn_src_seqs[0])\n",
    "print (trn_tgt_seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do the same for dev and test.\n",
    "dev_q_vecs, dev_a_vecs, dev_y = pickle.load(open('../data/twitter/tmp/de_dev.pkl', 'rb'))\n",
    "idxs = np.nonzero(dev_y)\n",
    "dev_src_seqs = dev_q_vecs[idxs[0]]\n",
    "dev_tgt_seqs = dev_a_vecs[idxs[0]]\n",
    "dev_src_seqs = add_seq2seq_tokens(dev_src_seqs, vocabulary[\"_BOS_\"], vocabulary[\"_EOS_\"])\n",
    "dev_tgt_seqs = add_seq2seq_tokens(dev_tgt_seqs, vocabulary[\"_BOS_\"], vocabulary[\"_EOS_\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q_vecs, test_a_vecs, test_y = pickle.load(open('../data/twitter/tmp/de_test.pkl', 'rb'))\n",
    "idxs = np.nonzero(test_y)\n",
    "test_src_seqs = test_q_vecs[idxs[0]]\n",
    "test_tgt_seqs = test_a_vecs[idxs[0]]\n",
    "test_src_seqs = add_seq2seq_tokens(test_src_seqs, vocabulary[\"_BOS_\"], vocabulary[\"_EOS_\"])\n",
    "test_tgt_seqs = add_seq2seq_tokens(test_tgt_seqs, vocabulary[\"_BOS_\"], vocabulary[\"_EOS_\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now safely save our inputs and output sequences to the tmp folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((trn_src_seqs, trn_tgt_seqs), open('../data/twitter/tmp/seq2seq_train.pkl', 'wb'))\n",
    "pickle.dump((dev_src_seqs, dev_tgt_seqs), open('../data/twitter/tmp/seq2seq_dev.pkl', 'wb'))\n",
    "pickle.dump((test_src_seqs, test_tgt_seqs), open('../data/twitter/tmp/seq2seq_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
